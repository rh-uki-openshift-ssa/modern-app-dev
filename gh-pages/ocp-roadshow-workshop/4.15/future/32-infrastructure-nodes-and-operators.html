<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Infrastructure Nodes and Operators :: OpenShift Roadshow Workshop 2024</title>
    <link rel="canonical" href="http://localhost:3000/rhs-openshift-starter-guides/index.html/ocp-roadshow-workshop/4.15/future/32-infrastructure-nodes-and-operators.html">
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../../_/css/site.css">
<link rel="icon" href="../../../_/img/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body class="article">
<header class="header">
  <script type="text/javascript">
    document.addEventListener('DOMContentLoaded', function () { 
      const urlParams = new URLSearchParams(window.location.search);
      const clusterName = urlParams.get('CLUSTER_SUBDOMAIN');
      const projectName = urlParams.get('PROJECT');
      if (clusterName) {
        showCluster( clusterName );
      }
      else {
        showClusterForm();
      }

      if (projectName) {
        showProject( projectName );
      } else {
        showProjectForm();
      }
    } );


    function sliceCluster(clusterName){
      if (clusterName.length > 70) {
          return "..." + clusterName.slice(35);
      } else {
          return clusterName;
      }
    }

    function showCluster( clusterName ) {
      document.getElementById('navbar-form-empty').style.display = "none";
      document.getElementById('navbar-form-filled').style.display = "flex";
      document.getElementById('cluster_subdomain').textContent = sliceCluster(clusterName);
      document.getElementById('clusterfield2').value = clusterName;

    }

    function showClusterForm( clusterName ) {
      document.getElementById('navbar-form-empty').style.display = "flex";
      document.getElementById('navbar-form-filled').style.display = "none";
    }

    function gowithcluster() {
      elClusterInput = document.getElementById('clusterfield');
      elProjectInput = document.getElementById('projectfield2');

      window.location.search = ('&CLUSTER_SUBDOMAIN=' + elClusterInput.value + '&PROJECT=' + elProjectInput.value);
    }

    function showProject( projectName ) {
      document.getElementById('navbar-form-project-empty').style.display = "none";
      document.getElementById('navbar-form-project-filled').style.display = "flex";
      document.getElementById('project').textContent = projectName;
      document.getElementById('projectfield2').value = projectName;
    }

    function showProjectForm( projectName ) {
      document.getElementById('navbar-form-project-empty').style.display = "flex";
      document.getElementById('navbar-form-project-filled').style.display = "none";
    }

    function gowithproject() {
      elProjectInput = document.getElementById('projectfield');
      elClusterInput = document.getElementById('clusterfield2');
      window.location.search = ('&CLUSTER_SUBDOMAIN=' + elClusterInput.value + '&PROJECT=' + elProjectInput.value);
    }

  </script>
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://developers.redhat.com" target="_blank"><img
          src="../../../_/img/header_logo.png" height="40px" alt="Red Hat Developer Program"></a>
      <a class="navbar-item" href="http://localhost:3000/rhs-openshift-starter-guides/index.html">OpenShift Roadshow Workshop 2024</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item" id="navbar-form-empty">
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">&nbsp;<i style="color: orange;"
              class="fa fa-exclamation-triangle" aria-hidden="true"></i></span>

          <form action="javascript:void(0);" onsubmit="gowithcluster();">
            <input size="40" id="clusterfield" type="text" placeholder="Enter Cluster Subdomain">
            <input type="hidden" id="projectfield2" name="projectfield2" value="">
          </form>
        </div>

        <div class="navbar-item" id="navbar-form-filled" style="display: none;">
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">|</span>
          <span class="navbar-text" id="cluster_subdomain"></span>
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">&nbsp;<i onclick="recycle();" style="color: green;" class="fa fa-recycle" aria-hidden="true"></i></span>
        </div>

         <div class="navbar-item" id="navbar-form-project-empty">
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">&nbsp;<i style="color: orange;"
              class="fa fa-exclamation-triangle" aria-hidden="true"></i></span>

          <form action="javascript:void(0);" onsubmit="gowithproject();">
            <input size="40" id="projectfield" type="text" placeholder="Enter Project Name">
            <input type="hidden" id="clusterfield2" name="clusterfield2" value="">
          </form>
        </div>

        <div class="navbar-item" id="navbar-form-project-filled" style="display: none;">
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">|</span>
          <span class="navbar-text" id="project"></span>
          <span class="navbar-text" style="margin-left: 1rem; margin-right: 1rem;">&nbsp;<i onclick="recycle();" style="color: green;" class="fa fa-recycle" aria-hidden="true"></i></span>
        </div>
<!---
        <a class="navbar-item" href="https://developers.redhat.com/ebooks/" target="_blank">Books</a>
        <a class="navbar-item" href="https://developers.redhat.com/cheatsheets/" target="_blank">Cheat Sheets</a>
        <a class="navbar-item" href="https://developers.redhat.com/events/" target="_blank">Upcoming Events</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Tutorials</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://redhat-scholars.github.io/kubernetes-tutorial/" target="_blank">Kubernetes</a>
            <a class="navbar-item" href="https://redhat-scholars.github.io/istio-tutorial/" target="_blank">Istio</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/quarkus-tutorial/" target="_blank">Quarkus</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/knative-tutorial/" target="_blank">Knative</a>
            <a class="navbar-item" href="https://redhat-scholars.github.io/tekton-tutorial/" target="_blank">Tekton</a>
          </div>
--->
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="ocp-roadshow-workshop" data-version="4.15">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html" class=" query-params-link">OCP Roadshow Workshop</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../xx-workshop-credentials-links.html">Credentials and links</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../10-security-infrastructure-introduction.html">Security &amp; Infrastructure</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../11-application-management-basics.html"><strong>04</strong> - Application Management Basics</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../12-application-storage-basics.html"><strong>05</strong> - Application Storage Basics</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../13-networking.html"><strong>06</strong> - Networking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../14-acs-vulnerability.html"><strong>07</strong> - ACS Vulnerability Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../15-acm.html"><strong>08</strong> - ACM multicluster Management</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCP Roadshow Workshop</span>
    <span class="version">4.15</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">OCP Roadshow Workshop</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">4.15</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCP Roadshow Workshop</a></li>
    <li><a href="32-infrastructure-nodes-and-operators.html">Infrastructure Nodes and Operators</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///home/marrober/data/git-repos/rhuki-openshift-ssa/modern-app-dev/documentation/modules/ROOT/pages/future/32-infrastructure-nodes-and-operators.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<article class="doc">
<h1 class="page">Infrastructure Nodes and Operators</h1>
<div class="sect1">
<h2 id="_openshift_infrastructure_nodes"><a class="anchor" href="#_openshift_infrastructure_nodes"></a>OpenShift Infrastructure Nodes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OpenShift components that fall into the infrastructure categorization include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>kubernetes and OpenShift control plane services ("masters")</p>
</li>
<li>
<p>router</p>
</li>
<li>
<p>container image registry</p>
</li>
<li>
<p>cluster metrics collection ("monitoring")</p>
</li>
<li>
<p>cluster aggregated logging</p>
</li>
<li>
<p>service brokers</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Any node running a container/pod/component not described above is considered a worker and must be covered by a subscription.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_more_machineset_details"><a class="anchor" href="#_more_machineset_details"></a>More MachineSet Details</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the MachineSets exercises you explored using MachineSets and scaling the cluster by changing their replica count. In the case of an infrastructure node, we want to create additional Machines that have specific Kubernetes labels. Then, we can configure the various infrastructure components to run specifically on nodes with those labels.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Currently the operators that are used to control infrastructure components do not all support the use of taints and tolerations. This means that infrastructure workload will go onto the infrastructure nodes, but other workload is not specifically prevented from running on the infrastructure nodes. In other words, user workload may commingle with infrastructure workload until full taint/toleration support is implemented in all operators.</p>
</div>
<div class="paragraph">
<p>Taints and tolerations work in tandem to ensure that workloads are not scheduled onto certian nodes. They’ll be explored later on.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To accomplish this, you will create additional MachineSets.</p>
</div>
<div class="paragraph">
<p>In order to understand how MachineSets work, run the following.</p>
</div>
<div class="paragraph">
<p>This will allow you to follow along with some of the following discussion.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">CLUSTERNAME=$(oc get  infrastructures.config.openshift.io cluster  -o jsonpath='{.status.infrastructureName}')
ZONENAME=$(oc get nodes -L topology.kubernetes.io/zone  --no-headers  | awk '{print $NF}' | tail -1)
oc get machineset -n openshift-machine-api -o yaml ${CLUSTERNAME}-worker-${ZONENAME}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_metadata"><a class="anchor" href="#_metadata"></a>Metadata</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The metadata on the MachineSet itself includes information like the name of the MachineSet and various labels.</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">metadata:
  creationTimestamp: 2019-01-25T16:00:34Z
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: 190125-3
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: 190125-3-worker-us-east-1b
  namespace: openshift-machine-api
  resourceVersion: "9027"
  selfLink: /apis/cluster.k8s.io/v1alpha1/namespaces/openshift-machine-api/machinesets/190125-3-worker-us-east-1b
  uid: 591b4d06-20ba-11e9-a880-068acb199400</code></pre>
</div>
</div>
<div class="paragraph">
<p>You might see some annotations on your MachineSet if you dumped one that had a MachineAutoScaler defined.
Selector
The MachineSet defines how to create Machines, and the Selector tells the operator which machines are associated with the set:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: 190125-3
      machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this case, the cluster name is 190125-3 and there is an additional label for the whole set.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_template_metadata"><a class="anchor" href="#_template_metadata"></a>Template Metadata</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The template is the part of the MachineSet that templates out the Machine. The template itself can have metadata associated, and we need to make sure that things match here when we make changes:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  template:
    metadata: {}
      labels:
        machine.openshift.io/cluster-api-cluster: 190125-3
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: 190125-3-worker-us-east-1b</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_template_spec"><a class="anchor" href="#_template_spec"></a>Template Spec</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The template needs to specify how the Machine/Node should be created. You will notice that the spec and, more specifically, the providerSpec contains all of the important AWS data to help get the Machine created correctly and bootstrapped.</p>
</div>
<div class="paragraph">
<p>In our case, we want to ensure that the resulting node inherits one or more specific labels. As you’ve seen in the examples above, labels go in metadata sections:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-08871aee06d13e584
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default the MachineSets that the installer creates do not apply any additional labels to the node.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_defining_a_custom_machineset"><a class="anchor" href="#_defining_a_custom_machineset"></a>Defining a Custom MachineSet</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that you’ve analyzed an existing MachineSet it’s time to go over the rules for creating one, at least for a simple change like we’re making:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Don’t change anything in the providerSpec</p>
</li>
<li>
<p>Don’t change any instances of machine.openshift.io/cluster-api-cluster: &lt;clusterid&gt;</p>
</li>
<li>
<p>Give your MachineSet a unique name</p>
</li>
<li>
<p>Make sure any instances of machine.openshift.io/cluster-api-machineset match the name</p>
</li>
<li>
<p>Add labels you want on the nodes to .spec.template.spec.metadata.labels</p>
</li>
<li>
<p>Even though you’re changing MachineSet name references, be sure not to change the subnet.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This sounds complicated, but we have a little script and some steps that will do the hard work for you:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">bash /opt/app-root/src/support/machineset-generator.sh 1 infra 0 | oc create -f -
export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'
oc scale machineset $MACHINESET -n openshift-machine-api --replicas=3</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then go ahead and run:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get machineset -n openshift-machine-api</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see the new infra set listed with a name similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
cluster-city-56f8-mc4pf-infra-us-east-2a    1         1                             13s
...</pre>
</div>
</div>
<div class="paragraph">
<p>We don’t yet have any ready or available machines in the set because the instances are still coming up and bootstrapping. You can check oc get machine -n openshift-machine-api to see when the instance finally starts running. Then, you can use oc get node to see when the actual node is joined and ready.</p>
</div>
<div class="paragraph">
<p>It can take several minutes for a Machine to be prepared and added as a Node.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get nodes</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-133-134.us-east-2.compute.internal   Ready    infra,worker   8m     v1.16.2
ip-10-0-133-191.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-136-83.us-east-2.compute.internal    Ready    master         67m    v1.16.2
ip-10-0-138-24.us-east-2.compute.internal    Ready    infra,worker   8m1s   v1.16.2
ip-10-0-139-81.us-east-2.compute.internal    Ready    infra,worker   8m3s   v1.16.2
ip-10-0-152-132.us-east-2.compute.internal   Ready    worker         61m    v1.16.2
ip-10-0-157-139.us-east-2.compute.internal   Ready    master         67m    v1.16.2
ip-10-0-167-9.us-east-2.compute.internal     Ready    worker         61m    v1.16.2
ip-10-0-169-121.us-east-2.compute.internal   Ready    master         67m    v1.16.2</pre>
</div>
</div>
<div class="paragraph">
<p>If you’re having trouble figuring out which node is the new one, take a look at the AGE column. It will be the youngest! Also, in the ROLES column you will notice that the new node has both a worker and an infra role.</p>
</div>
<div class="paragraph">
<p>Alternatively you can list the node by role.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get nodes -l node-role.kubernetes.io/infra</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_check_the_labels"><a class="anchor" href="#_check_the_labels"></a>Check the Labels</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In our case, the youngest node was named ip-10-0-128-138.us-east-1.compute.internal, so we can ask what its labels are:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">YOUNG_INFRA_NODE=$(oc get nodes -l node-role.kubernetes.io/infra  --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[0].metadata.name}')
oc get nodes ${YOUNG_INFRA_NODE} --show-labels | grep --color node-role</code></pre>
</div>
</div>
<div class="paragraph">
<p>And, in the LABELS column we see:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos</pre>
</div>
</div>
<div class="paragraph">
<p>It’s hard to see, but our node-role.kubernetes.io/infra label is there.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_add_more_machinesets_or_scale_or_both"><a class="anchor" href="#_add_more_machinesets_or_scale_or_both"></a>Add More Machinesets (or scale, or both)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In a realistic production deployment, you would want at least 3 MachineSets to hold infrastructure components. Both the logging aggregation solution and the service mesh will deploy ElasticSearch, and ElasticSearch really needs 3 instances spread across 3 discrete nodes. Why 3 MachineSets? Well, in theory, having multiple MachineSets in different AZs ensures that you don’t go completely dark if AWS loses an AZ.</p>
</div>
<div class="paragraph">
<p>The MachineSet you created with the scriptlet already created 3 replicas for you, so you don’t have to do anything for now. Don’t create any additional ones yourself, either — the AWS limits on the account you are using are purposefully small.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_extra_credit"><a class="anchor" href="#_extra_credit"></a>Extra Credit</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the openshift-machine-api project are several Pods. One of them has a name like machine-api-controllers-56bdc6874f-86jnb. If you use oc logs on the various containers in that Pod, you will see the various operator bits that actually make the nodes come into existence.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_quick_operator_background"><a class="anchor" href="#_quick_operator_background"></a>Quick Operator Background</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Operators are just Pods. But they are special Pods. They are software that understands how to deploy and manage applications in a Kubernetes environment. The power of Operators relies on a Kubernetes feature called CustomResourceDefinitions (CRD). A CRD is exactly what it sounds like. They are a way to define a custom resource which is essentially extending the Kubernetes API with new objects.</p>
</div>
<div class="paragraph">
<p>If you wanted to be able to create/read/update/delete Foo objects in Kubernetes, you would create a CRD that defines what a Foo resource is and how it works. You can then create CustomResources (CRs) — instances of your CRD.</p>
</div>
<div class="paragraph">
<p>With Operators, the general pattern is that an Operator looks at CRs for its configuration, and then it operates on the Kubernetes environment to do whatever the configuration specifies. Now you will take a look at how some of the infrastructure operators in OpenShift do their thing.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_moving_infrastructure_components"><a class="anchor" href="#_moving_infrastructure_components"></a>Moving Infrastructure Components</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that you have some special nodes, it’s time to move various infrastructure components onto them.</p>
</div>
<div class="sect2">
<h3 id="_router"><a class="anchor" href="#_router"></a>Router</h3>
<div class="paragraph">
<p>The OpenShift router is managed by an Operator called openshift-ingress-operator. Its Pod lives in the openshift-ingress-operator project:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pod -n openshift-ingress-operator</code></pre>
</div>
</div>
<div class="paragraph">
<p>The actual default router instance lives in the openshift-ingress project. Take a look at the Pods.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -n openshift-ingress -o wide</code></pre>
</div>
</div>
<div class="paragraph">
<p>And you will see something like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   &lt;none&gt;
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   &lt;none&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Review a Node on which a router is running:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">ROUTER_POD_NODE=$(oc get pods -n openshift-ingress -o jsonpath='{.items[0].spec.nodeName}')
oc get node ${ROUTER_POD_NODE}</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see that it has the role of worker.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1</pre>
</div>
</div>
<div class="paragraph">
<p>The default configuration of the router operator is to pick nodes with the role of worker. But, now that we have created dedicated infrastructure nodes, we want to tell the operator to put the router instances on nodes with the role of infra.</p>
</div>
<div class="paragraph">
<p>The OpenShift router operator uses a custom resource definition (CRD) called ingresses.config.openshift.io to define the default routing subdomain for the cluster:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get ingresses.config.openshift.io cluster -o yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>The cluster object is observed by the router operator as well as the master. Yours likely looks something like:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: apps.cluster-kswh5.kswh5.sandbox1208.opentlc.com
status: {}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Individual router deployments are managed via the ingresscontrollers.operator.openshift.io CRD. There is a default one created in the openshift-ingress-operator namespace:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Yours looks something like:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
----
To specify a nodeSelector that tells the router pods to hit the infrastructure nodes, we can apply the following configuration:</code></pre>
</div>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f /opt/app-root/src/support/ingresscontroller.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>You may see an error that says Warning: resource is missing the kubectl.kubernetes.io/last-applied-config. This is normal, an apply envokes a "3 way diff merge" on the resource. Since the ingress controller was only just created on install, there was no "last applied" configuration for it. If you run that command again, you shouldn’t see that warning.
Run:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pod -n openshift-ingress -o wide</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Your session may timeout during the router move. Please refresh the page to get your session back. You will not lose your terminal session but may have to navigate back to this page manually.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If you’re quick enough, you might catch either Terminating or ContainerCreating pods. The Terminating pod was running on one of the worker nodes. The Running pods eventually are on one of our nodes with the infra role.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_registry"><a class="anchor" href="#_registry"></a>Registry</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The registry uses a similar CRD mechanism to configure how the operator deploys the actual registry pods. That CRD is configs.imageregistry.operator.openshift.io. You will edit the cluster CR object in order to add the nodeSelector. First, take a look at it:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get configs.imageregistry.operator.openshift.io/cluster -o yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see something like:</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you run the following command:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge</code></pre>
</div>
</div>
<div class="paragraph">
<p>It will modify the .spec of the registry CR in order to add the desired nodeSelector.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>At this time the image registry is not using a separate project for its operator. Both the operator and the operand are housed in the openshift-image-registry project.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After you run the patch command you should see the registry pod being moved to the infra node. The registry is in the openshift-image-registry project. If you execute the following quickly enough:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pod -n openshift-image-registry</code></pre>
</div>
</div>
<div class="paragraph">
<p>You might see the old registry pod terminating and the new one starting. Since the registry is being backed by an S3 bucket, it doesn’t matter what node the new registry pod instance lands on. It’s talking to an object store via an API, so any existing images stored there will remain accessible.</p>
</div>
<div class="paragraph">
<p>Also note that the default replica count is 1. In a real-world environment you might wish to scale that up for better availability, network throughput, or other reasons.</p>
</div>
<div class="paragraph">
<p>If you look at the node on which the registry landed (see the section on the router), you’ll note that it is now running on an infra worker.</p>
</div>
<div class="paragraph">
<p>Lastly, notice that the CRD for the image registry’s configuration is not namespaced — it is cluster scoped. There is only one internal/integrated registry per OpenShift cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_monitoring"><a class="anchor" href="#_monitoring"></a>Monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Cluster Monitoring operator is responsible for deploying and managing the state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is installed by default during the initial cluster installation. Its operator uses a ConfigMap in the openshift-monitoring project to set various tunables and settings for the behavior of the monitoring stack.</p>
</div>
<div class="paragraph">
<p>The following ConfigMap definition will configure the monitoring solution to be redeployed onto infrastructure nodes.</p>
</div>
<div class="listingblock console">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""</code></pre>
</div>
</div>
<div class="paragraph">
<p>There is no ConfigMap created as part of the installation. Without one, the operator will assume default settings. Verify the ConfigMap is not defined in your cluster:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get configmap cluster-monitoring-config -n openshift-monitoring</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Error from server (NotFound): configmaps "cluster-monitoring-config" not found</pre>
</div>
</div>
<div class="paragraph">
<p>The operator will, in turn, create several ConfigMap objects for the various monitoring stack components, and you can see them, too:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get configmap -n openshift-monitoring</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can create the new monitoring config with the following command:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc create -f /opt/app-root/src/support/cluster-monitoring-configmap.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Watch the monitoring pods move from worker to infra Nodes with:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">watch 'oc get pod -n openshift-monitoring'</code></pre>
</div>
</div>
<div class="paragraph">
<p>or:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pod -w -n openshift-monitoring</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can exit by pressing kbd:[Ctrl+C].</p>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <a class="rhd-logo" href="https://developers.redhat.com" target="_blank"></div>
</footer>
<script src="../../../_/js/vendor/clipboard.js"></script>
<script src="../../../_/js/site.js"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
